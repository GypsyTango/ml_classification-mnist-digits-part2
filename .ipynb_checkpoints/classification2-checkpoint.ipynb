{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import os, gzip, shutil, codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/clou/Desktop/git_control/mtu_ml_proj_team/classification/classification2/raw_data//extracted/t10k-images-idx3-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-32023ba42580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_in\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_out\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mfiles_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/clou/Desktop/git_control/mtu_ml_proj_team/classification/classification2/raw_data//extracted/t10k-images-idx3-ubyte'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since this project uses the same mnist data set, we adapted \n",
    "the first classification project to extract our data\n",
    "\n",
    "\"\"\"\n",
    "path_in = os.getcwd()+'/raw_data/'\n",
    "path_out = os.getcwd()+'/extracted/'\n",
    "# path_in = os.getcwd()+'\\\\extracted\\\\'\n",
    "# path_out = path_in\n",
    "\n",
    "files_in = os.listdir(path_in)\n",
    "# print(\"original data: \", files_in)\n",
    "    \n",
    "for file in files_in:\n",
    "    if file.endswith('gz'):\n",
    "        with gzip.open(path_in + file, mode='rb') as f_in:\n",
    "            with open(path_out + file.split('.')[0], mode='wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "                files_out = os.listdir(path_out)\n",
    "\n",
    "my_dic = {}\n",
    "for file1 in files_out:\n",
    "    if file1.endswith('ubyte'):\n",
    "        with open(path_out + file1, mode='rb') as f:\n",
    "            my_data = f.read()\n",
    "            # file type check\n",
    "            magic_num = int(codecs.encode(my_data[0:4], 'hex'), 16)\n",
    "            # file length check\n",
    "            num_len = int(codecs.encode(my_data[4:8], 'hex'), 16)\n",
    "            # print(num_len, magic_num)\n",
    "            # train len: 60000; test len: 10000 \n",
    "            if num_len == 10000:\n",
    "                length = 'test'\n",
    "            elif num_len == 60000:\n",
    "                length = 'train'\n",
    "                \n",
    "            if magic_num == 2049:\n",
    "                # start from offset 8, convert one label to an integer each time\n",
    "                lab = np.frombuffer(my_data, np.uint8, offset=8)\n",
    "                parse = lab.reshape(num_len)\n",
    "                # print(lab)\n",
    "                cat = 'label'\n",
    "            elif magic_num == 2051:\n",
    "                # rows from offset 8 - 11\n",
    "                rows = int(codecs.encode(my_data[8:12], 'hex'), 16)\n",
    "                # colmns from offset 12 - 15\n",
    "                cols = int(codecs.encode(my_data[12:16], 'hex'), 16)\n",
    "                # start from offset 16, convert one pixel to an integer each time\n",
    "                pix = np.frombuffer(my_data, np.uint8, offset=16)\n",
    "                parse = pix.reshape(num_len, rows, cols)\n",
    "                cat = 'image'\n",
    "            \n",
    "            my_dic[cat + '_' + length] = parse\n",
    "\n",
    "print(my_dic.keys())\n",
    "print('image_test shape: ' + str(my_dic['image_test'].shape))\n",
    "print('label_test shape: ' + str(my_dic['label_test'].shape))\n",
    "print('image_train shape: ' + str(my_dic['image_train'].shape))\n",
    "print('label_train shape: ' + str(my_dic['label_train'].shape))\n",
    "# my_dic['label_test']\n",
    "X_train, y_train = my_dic['image_train'].reshape(60000, 28*28), my_dic['label_train']\n",
    "X_test, y_test = my_dic['image_test'].reshape(10000, 28*28), my_dic['label_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***training and testing data dimentions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape: ' + str(X_train.shape))\n",
    "print('X_test shape: ' + str(X_test.shape))\n",
    "print('y_train shape: ' + str(y_train.shape))\n",
    "print('y_test shape: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. \n",
    "Since the model for each pixel follows Gaussian distribution, it means the prior probability of each class is the same. That is, $\\ P(c=0) = P(c=1) = P(c=2) = ... = P(c=9)$. Then, the proir probablity ends up to be a constant factor and we will not consider this term\n",
    "for our classifier. Therefore, $\\ P(c|x)=\\prod_{i=1}^{28 \\times28}P(x_i|c)$ is the model we are using, which is the product of each image pixel's probability given the label class from 0 to 9.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to consider about the smoothing parameter by adding some value k to our model. This is because we cannot guarantee our training data have all the label classes. It might occur that the number of image pixels given the specific label is 0. When it happens, Our result will always return to 0 no matter what. So, adding a smoothing parameter is very important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    \n",
    "    \n",
    "    '''no prior terms are considered for our classifier'''\n",
    "    def fit(self, X, y, smoothing=10e-3):\n",
    "        self.gaussians = {}\n",
    "        labels = set(y)\n",
    "        for c in labels:\n",
    "            x = X[y == c]\n",
    "            self.gaussians[c] = {\n",
    "                'mean': x.mean(axis=0),\n",
    "                'var': x.var(axis=0) + smoothing\n",
    "            }\n",
    "            \n",
    "        \n",
    "    def cm_accuracy(self, X, y):\n",
    "        P = self.predict(X)[0]\n",
    "        cfm = confusion_matrix(P, y)\n",
    "        overall_accuracy = round(sum(np.diag(cfm)) / len(y), 5)\n",
    "        accuracy = []\n",
    "        for i in range(10):\n",
    "            a = round(np.diag(cfm)[i] / sum(cfm[i]), 5)\n",
    "            accuracy.append(a)\n",
    "        return (cfm, overall_accuracy, accuracy)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        N, D = X.shape\n",
    "        K = len(self.gaussians)\n",
    "        P = np.zeros((N, K))\n",
    "        mean_list = []\n",
    "        var_list = []\n",
    "        # prediction        \n",
    "        for c , g in self.gaussians.items():\n",
    "            mean, var = g['mean'], g['var']\n",
    "            mean_list.append(mean)\n",
    "            var_list.append(var)\n",
    "            P[:, c] = mvn.logpdf(X, mean = mean, cov = var)\n",
    "            np.argmax(P, axis=1)\n",
    "        return np.argmax(P, axis=1), np.argmax(P, axis=0), mean_list, var_list\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def pred_real_plt(self, X):\n",
    "        tmp = self.predict(X)[0]\n",
    "        iterate = 0\n",
    "        '''\n",
    "        We can cutomize the number of test time to observe the accuracy of the conditional \n",
    "        probability intuitively. We used the range of 4 in this case\n",
    "        \n",
    "        '''\n",
    "        for i in range(4):\n",
    "            labels = [\"P(x|c=\"+str(tmp[0+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[1+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[2+iterate])+\")\",\n",
    "                      \"P(x|c=\"+str(tmp[3+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[4+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[5+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[6+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[7+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[8+iterate])+\")\", \n",
    "                      \"P(x|c=\"+str(tmp[9+iterate])+\")\"]\n",
    "            plt.subplots(figsize=(20, 5))\n",
    "            for y, cls in enumerate(labels):\n",
    "                plt.subplot(1, len(labels), y + 1)\n",
    "                plt.imshow(X[y+iterate].reshape((28, 28)), cmap='gray')\n",
    "                plt.title(cls)\n",
    "                plt.axis(\"off\")\n",
    "            '''\n",
    "            as long as the iteration is the factor of 10, we can secure the prediction value \n",
    "            '''\n",
    "            iterate += 300\n",
    "            \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    model = NaiveBayes()\n",
    "    print('Number of training set used: ' + str(len(y_train)))\n",
    "    t0 = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    print('NaiveBayes model training time: ', round(time() - t0, 3), 's')\n",
    "    print(\"Overall train confusion matrix: \\n\", model.cm_accuracy(X_train, y_train)[0])\n",
    "    t0 = time()\n",
    "    print(\"Overall train accuracy: \", model.cm_accuracy(X_train, y_train)[1])\n",
    "    print(\"Time spent in overall train accuracy: \", round(time() - t0, 3), 's')\n",
    "    t0 = time()\n",
    "    print(\"Each label's train accuracy: \", pd.DataFrame(model.cm_accuracy(X_train, y_train)[2], columns=[\" \"]))\n",
    "    print(\"Time spent for each train accuracy: \", round(time() - t0, 3), 's')\n",
    "    \n",
    "    print('==============================================================')\n",
    "    \n",
    "    print('Number of test set used: ' + str(len(y_test)))\n",
    "    print(\"Overall test confusion matrix: \\n\", model.cm_accuracy(X_test, y_test)[0])\n",
    "    t0 = time()\n",
    "    print(\"Overall test accuracy : \", model.cm_accuracy(X_test, y_test)[1])\n",
    "    print(\"Time spent in overall test accuracy: \", round(time() - t0, 3), 's')\n",
    "    t0 = time()\n",
    "    print(\"Each label's test accuracy: \", pd.DataFrame(model.cm_accuracy(X_test, y_test)[2], columns=[\" \"]))\n",
    "    print(\"Time for in each test accuracy: \", round(time() - t0, 3), 's')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy seems low, with only ~61% in this classifier. However, the individual label accuracy seems high except digits 8 and 9. I assume the reason of the high individual label accuracy is because each digit have different sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict() function predicts each digit's label pixel by pixel (784 pixels), then it returns the most likely digit for the result by applying multivariant normal distribution probability density function. This result is going to compare with the true labels from the testing data set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test is a set of truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image results using conditional probability for each pixel to classify labels are shown below. By using this classifier, we are able to use each of those image to compare with all 0s, 1s, ... in the testing data set. If the majority of the pixels on the testing data set are hitting the one of these classifier, it will classify that as a corresbonding class label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mean = model.predict(X_test)[2]\n",
    "\n",
    "f, axs = plt.subplots(2, 5,figsize=(15, 6), sharey=True)\n",
    "axs = axs.ravel()\n",
    "for i in range(len(w_mean)):\n",
    "    axs[i].imshow(w_mean[i].reshape(28,28),cmap=plt.cm.gray)\n",
    "    axs[i].set_title('P(x|c= %s'%i+\")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each digit we are comparing below, it seems like the classifier is easily to be tricked to predict 8 and 9 with a 7. It might be the cause of people's writing habbits. For instance, from the 2nd row 2nd column and 8th column, both of them are digit 7; but some people would like to add a little bar on this digit, and therefore it somehow confuses the classifier to make the right choice. Digit 1 and 0 seems fine from these images and it makes sense because our test accuracy for 1 and 0 are relatively high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pred_real_plt(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. (60 points) Build a regularized logistic regression classiﬁer, where you use ridge (2) regularization. Test this classiﬁer on the MNIST data set by developing 10 classiﬁers: 0 versus all, 1 versus all, 2 versus all, ... , 9 versus all. Provide a confusion matrix, accuracy for each digit, and overall accuracy. Plot the overall test accuracy versus the regularization value where a log-scale is used for regularization value. Essentially, the ‘1 versus all’ classiﬁer is trained to give you a probability of the digit 1 versus all other digits. Hence, digit 1 is class +1 and all other digits are class -1. Hence, to classify a test image, you take the maximum probability from all 10 classiﬁers, giving the predicted class of the input image. 2 regularized logistic regression uses the following log-likelihood,\n",
    "L(w) =\n",
    "N X i=1\n",
    "log(1 + exp(−yiwTxi)) + λ 2kwk2. Recall, that by taking the gradient of L(w), we obtained the following gradient descent update equation for (non-regularized) logistic regression, wt+1 = wt −ηt∇L(w), where ∇L(w) = N X i=1 −yixi exp(−yiwTxi) 1 + exp(−yiwTxi) . Derive the update equation for the regularized logistic regression and present your derivation. Apply that classiﬁer to the MNIST data set. Don’t forget to add a column of 1s to your 784-length feature vector (the image values) so that you get the bias term with your classiﬁer. For each of your 10 trained classiﬁers, show an image of the 784 weights (don’t show the bias weight) as a 28 × 28 image. Do these images provide any insight to how this classiﬁer works? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, signal):\n",
    "        '''\n",
    "        The sigmoid is a function that has the equation 1/(1+e^-x).\n",
    "        The sigmoid is used within the function for both the computation and final\n",
    "        prediction of the algorithm. We will treat P(y|x) = sigmoid(y * w.T * x)\n",
    "        '''\n",
    "        return 1/(1 + np.exp(-signal))\n",
    "\n",
    "\n",
    "    def gradient(self, features,labels, lam, weight,learn_rate):\n",
    "        #Simplifys the step of matrix multiplication\n",
    "        feature_label = np.multiply(features,labels.reshape(-1,1))  #X*y\n",
    "        features_weight = np.dot(features,weight)  #X*w.T\n",
    "        \n",
    "        #Calculates class error then multiplys through by XY\n",
    "        class_error = (1 - self.sigmoid(np.multiply(features_weight,labels))).reshape(-1,1)  #1 - Probability(y|x)\n",
    "        grade = np.multiply(feature_label,class_error)  #XY ( 1-Probability(y|x))\n",
    "        return learn_rate * (-sum(grade) + lam*weight)   #n * (-sum(xy(1-p(y|x))) + lam*w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def normal_gradient(self, features,labels,lam,add_intercept = False):\n",
    "        '''\n",
    "        This section will apply the update step 10 times for w using a specified \n",
    "        learning rate. Rather than using the learning rate of inverse square root\n",
    "        we decided to go with just the inverse because it would lead to a sharper \n",
    "        learning cut off. This was done because we don't have the luxury of using\n",
    "        more than 10 steps, due to computational complexity. \n",
    "        '''\n",
    "        if add_intercept:\n",
    "            intercept = np.ones((features.shape[0], 1))\n",
    "            features = np.hstack((intercept, features))\n",
    "        w = np.ones(features.shape[1])\n",
    "        #Cap the convergence at 10, this is due mainly to time constraints.\n",
    "        for i in range(10):\n",
    "            w = w - self.gradient(features,labels, lam, w,(1+i)**-1)\n",
    "        return w\n",
    "    \n",
    "    \n",
    "    ''' labeling model +1/ -1 '''\n",
    "    def label_model(self, y):\n",
    "        #We go through and add 10 label sets, one for each digit.\n",
    "        #The label sets are a +1 if the digit is the desired digit, \n",
    "        #and a -1 if its anything else.\n",
    "        inital = []\n",
    "        for i in range(10):\n",
    "            tmp = np.zeros(len(y)) - 1\n",
    "            tmp[np.where(y == i)] = 1\n",
    "            inital.append(tmp)\n",
    "        return inital\n",
    "    \n",
    "    \n",
    "    def weights(self, X, y, lam):\n",
    "        #The weight vectors are added to a list and outputted in this function.\n",
    "        mylist = []\n",
    "        for i in range(10):\n",
    "            tmp  = self.normal_gradient(X, self.label_model(y)[i], lam, add_intercept = True)\n",
    "            mylist.append(tmp)\n",
    "        return mylist\n",
    "            \n",
    "        \n",
    "    def prediction(self, X, y):\n",
    "        '''\n",
    "        We do the classification for a number of lambdas in this function.\n",
    "        The classification is done by taking the argmax of the prediction set.\n",
    "        Rather than plugging this into a sigmoid, we simply took the maximum\n",
    "        weight and declared that the classification. \n",
    "        This is similar to classifying by W.T*X rather than 1/1+e(-W.T*x)\n",
    "        Again, this is to simplify the computational complexity of the problem.\n",
    "        '''\n",
    "        lams = [0.001,0.01,0.1,1,10,100]\n",
    "        pred = []\n",
    "        for i in range(len(lams)):\n",
    "            w_set = self.weights(X, y, lam = lams[i])\n",
    "            for n in range(len(y_test)):\n",
    "                wt_x = []\n",
    "                for vector in w_set:\n",
    "                    wt_x.append(np.dot(vector[1:].reshape(1,-1),X_test[n].reshape(-1,1)))\n",
    "                pred.append(np.argmax(wt_x))            \n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def lambda_pick(self, X, y):\n",
    "        '''\n",
    "        This section goes through and takes the confusion matrix for each lambda\n",
    "        then picks the lambda with the best accuracy. That lambda is used for the\n",
    "        final confusion matrix.\n",
    "        '''\n",
    "        oa = []\n",
    "        for j in range(0, len(y_train), len(y_test)):\n",
    "            P = self.prediction(X, y)[j: j+len(y_test)]\n",
    "            cfm = confusion_matrix(P, y)\n",
    "            overall_accuracy = round(sum(np.diag(cfm)) / len(y), 5)\n",
    "            oa.append(overall_accuracy)\n",
    "        best_oa = max(oa)\n",
    "        return oa\n",
    "\n",
    "    \n",
    "    def prediction_refine(self, X, y, lam = 1):\n",
    "        '''\n",
    "        Since 1 was the best lambda for accuracy as we saw, we use a lambda of 1\n",
    "        in order to create the final set of predictions.\n",
    "        '''\n",
    "        lams = [1]\n",
    "        pred = []\n",
    "        for i in range(len(lams)):\n",
    "            w_set = self.weights(X_train, y_train, lam)\n",
    "            for n in range(len(y_test)):\n",
    "                wt_x = []\n",
    "                for vector in w_set:\n",
    "                    wt_x.append(np.dot(vector[1:].reshape(1,-1),X_test[n].reshape(-1,1)))\n",
    "                pred.append(np.argmax(wt_x))            \n",
    "        return pred\n",
    "\n",
    "    \n",
    "    def cm_accuracy(self, X, y):\n",
    "        #Calculates the confusion matrix, digit accuracy, and overall accuracy using\n",
    "        #the best value of lambda. In our case it was lam = 1.\n",
    "        P = self.prediction_refine(X, y)\n",
    "        cfm = confusion_matrix(P, y)\n",
    "        overall_accuracy = round(sum(np.diag(cfm)) / len(y), 5)\n",
    "        accuracy = []\n",
    "        for i in range(10):\n",
    "            a = round(np.diag(cfm)[i] / sum(cfm[i]), 5)\n",
    "            accuracy.append(a)\n",
    "        return (cfm, accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "accuracies = model_lr.lambda_pick(X_test, y_test)\n",
    "print('This analysis took: ', round(time() - t0, 3), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambdas tried values $\\ [0.001,0.01,0.1,1,10,100]$ to test the overall accuracy, lambda = 1 makes the best overall accuracy with 0.7895. This lambda test may take a while to analyze. The following accuracy and confusion matrix are based on lambda = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0.001, 0.01, 0.1, 1, 10, 100], accuracies, '--go')\n",
    "plt.suptitle('Accuracy VS Lambda')\n",
    "plt.xlabel('log of lambdas')\n",
    "plt.ylabel('accuracies')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above graph, there is a sweet spot for the regularization constant to get maximum accuracy. From testing, we found that a lambda value of 1 lead to a peak in our accuracy. \n",
    "\n",
    "However, it is important to note that due to time constraints the maximum number of iterations each lambda could be done was 10. This probably meant that for lower values of lambda there wasn't enough iterations in order to see the real difference in accuracy between these values. This lead to a accuracy that was roughly constant in the lower lambda range (lambda < 1) and as our regularization constant grew past 1 the accuracy dropped sharply. \n",
    "\n",
    "In the future, with more computational power we'd be interested in getting a higher resolution version of the above graph where we are able to test more lambdas in a smaller range around the peak value of 1. Overall, it is worth noting, the accuracy at each regularization apart from 100 was pretty good with values around 70%+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_lr.cm_accuracy(X_test, y_test)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to the confusion matrix you see above it is worth first noting the the row's indicate that the actual image is the index of the row. For instance, the 5th row shows the predictions when the image is ACTUALLY a 5. The columns indicate what value was predicted for a given row. So the 3rd row, 4th column would be the number of times the image was actually a 2 but was predicted to be a 3. \n",
    "Also worth noting, this confusion matrix was generated for a regularization constant of 1 because this lambda led to the highest overall accuracy.\n",
    "\n",
    "As you can see the majority of predictions matched that of the image, this is shown by the diagonal of the matrix having larger values than any non diagonal entry. Interestingly, for images of the digit 8 the algorithm had a very hard time predicting these images. This can be seen by how in the 9th row there are many large values for each column. Basically saying that the image 8 was misclassified not only frequently, but was misclassified as each other number. So it's safe to say that our algorithm had no idea what to do with images of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_lr.cm_accuracy(X_test, y_test)[1], columns=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the actual accuracies of each digit in the above table. Our most accurately classified digit was a 1 while the worst was an 8. This was in contrast to the naive bayes classifier where the most accurate digit was a 2 while the worst was still an 8. It is worth noting though that the Knn classifier had a similar problem with classifying 8. So it's safe to say that, for most classifiers, the digit 8 is a hard digit for most algorithms to guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_set = model_lr.weights(X_train, y_train, lam=1)\n",
    "\n",
    "f, axs = plt.subplots(2, 5,figsize=(15, 6), sharey=True)\n",
    "axs = axs.ravel()\n",
    "for i in range(len(w_set)):\n",
    "    axs[i].imshow(w_set[i][1:].reshape(28,28),cmap=plt.cm.gray)\n",
    "    axs[i].set_title('W %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we go through and plot the actual values of our W it's pretty clear that for each W we can tell what number is being represented. The only exceptions to this would be the digit 5 and 2, at least to me both of those digits are hard to tell from the above image. \n",
    "\n",
    "Part of the reason why I believe that 8 has so much trouble classifying images is because the digit 8 puts priority on the same pixels as the number 1, 3, 5, and 9. This is due to in the image the brighter pixels were the same in each of those digits. We can also see this within the confusion matrix, as the majority of the misclassifications for the images of the digit 8 happened to be 1's, 3's, 5's, and 9's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
